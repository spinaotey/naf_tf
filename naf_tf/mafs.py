import numpy as np
import tensorflow as tf
from naf_tf.flows import IAF_DSF,IAF_DDSF

dtype = tf.float32

class MaskedAutoregressiveFlowDSF:
    """
    Implements a Masked Autoregressive Flow, which is a stack of mades such that the random numbers which drive made i
    are generated by made i-1. The first made is driven by standard gaussian noise. In the current implementation, all
    mades are of the same type. If there is only one made in the stack, then it's equivalent to a single made.
    """

    def __init__(self, dim, hid_dim, context_dim, num_layers, num_flows,
                 activation=tf.nn.elu, output_order='sequential', mode='sequential',
                 num_ds_dim=4, num_ds_layers=1, num_ds_multiplier=3,
                 input=None,context=None):
    
        self.dim = dim
        self.context_dim = context_dim
        self.num_layers = num_layers
        self.num_flows = num_flows
        self.num_ds_dim = num_ds_dim
        self.num_ds_layers = num_ds_layers

        self.input = tf.placeholder(dtype=dtype,shape=[None,dim],name='x') if input is None else input
        if context_dim == 0:
            self.context = tf.reshape(tf.fill(tf.shape(self.input)[:1], np.float32(1.)),[-1,1])
        else:
            self.context = tf.placeholder(dtype=dtype,shape=[None,context_dim],name='context') if context is None\
                                                                                               else context
        self.logdet = tf.reshape(tf.fill(tf.shape(self.input)[:1], np.float32(0.)),[-1,1])

        self.parms = list()
        self.flows = list()
        self.u = self.input

        for i in range(num_flows):

            # create a new flow
            flow = IAF_DSF(dim, hid_dim, context_dim, num_layers,
                 activation, output_order, mode,
                 num_ds_dim, num_ds_layers, num_ds_multiplier,
                 input=self.u,context=self.context)
            self.flows.append(flow)
            self.parms += flow.parms
            
            output_order = output_order if output_order == 'random' else flow.model.output_order[::-1]
            self.u = flow.output
            self.logdet += flow.logdet

        # log likelihoods
        self.output = self.u
        self.L = -0.5 * dim * np.log(2 * np.pi) - 0.5 * tf.reduce_sum(self.output ** 2, axis=1,keepdims=True) + \
                 self.logdet

        # train objective
        self.trn_loss = -tf.reduce_mean(self.L,name='trn_loss')
        
    def eval(self, X, sess, log=True,batch_size=1000):
        """
        Evaluate log probabilities for given input-context pairs.
        :param X: a pair (x, y) where x rows are inputs and y rows are context variables
        :param sess: tensorflow session where the graph is run
        :param log: whether to return probabilities in the log domain
        :param batch_size: number of samples to be evaluated in one batch
        :return: log probabilities: log p(y|x)
        """
        
        if context_dim == 0:
            n = len(X)//batch_size
            lprob = []
            for i in range(n):
                lprob.append(sess.run(self.L,feed_dict={self.input:x[i*batch_size:(i+1)*batch_size]}))

            if n != len(y)/batch_size:
                lprob.append(sess.run(self.L,feed_dict={self.input:X[n*batch_size:]}))
            
        else:
            x, y = xy
            n = len(y)//batch_size
            lprob = []
            for i in range(n):
                lprob.append(sess.run(self.L,feed_dict={self.input:y[i*batch_size:(i+1)*batch_size],
                                                        self.context:x[i*batch_size:(i+1)*batch_size]}))

            if n != len(y)/batch_size:
                lprob.append(sess.run(self.L,feed_dict={self.input:y[n*batch_size:],
                                                        self.context:x[n*batch_size:]}))
        
        lprob = np.row_stack(lprob)

        return lprob if log else np.exp(lprob)
    
class MaskedAutoregressiveFlowDDSF:
    """
    Implements a Masked Autoregressive Flow, which is a stack of mades such that the random numbers which drive made i
    are generated by made i-1. The first made is driven by standard gaussian noise. In the current implementation, all
    mades are of the same type. If there is only one made in the stack, then it's equivalent to a single made.
    """

    def __init__(self, dim, hid_dim, context_dim, num_layers, num_flows,
                 activation=tf.nn.elu, output_order='sequential', mode='sequential',
                 num_ds_dim=4, num_ds_layers=1, num_ds_multiplier=3,
                 input=None,context=None):
    
        self.dim = dim
        self.context_dim = context_dim
        self.num_layers = num_layers
        self.num_flows = num_flows
        self.num_ds_dim = num_ds_dim
        self.num_ds_layers = num_ds_layers

        self.input = tf.placeholder(dtype=dtype,shape=[None,dim],name='x') if input is None else input
        if context_dim == 0:
            self.context = tf.reshape(tf.fill(tf.shape(self.input)[:1], np.float32(1.)),[-1,1])
        else:
            self.context = tf.placeholder(dtype=dtype,shape=[None,context_dim],name='context') if context is None\
                                                                                               else context
        self.logdet = tf.reshape(tf.fill(tf.shape(self.input)[:1], np.float32(0.)),[-1,1])

        self.parms = list()
        self.flows = list()
        self.u = self.input

        for i in range(num_flows):

            # create a new flow
            flow = IAF_DDSF(dim, hid_dim, context_dim, num_layers,
                 activation, output_order, mode,
                 num_ds_dim, num_ds_layers, num_ds_multiplier,
                 self.u,self.context)
            self.flows.append(flow)
            self.parms += flow.parms
            
            output_order = output_order if output_order == 'random' else flow.model.output_order[::-1]
            self.u = flow.output
            self.logdet += flow.logdet

        # log likelihoods
        self.output = self.u
        self.L = -0.5 * dim * np.log(2 * np.pi) - 0.5 * tf.reduce_sum(self.output ** 2, axis=1,keepdims=True) + \
                 self.logdet

        # train objective
        self.trn_loss = -tf.reduce_mean(self.L,name='trn_loss')
        
    def eval(self, X, sess, log=True,batch_size=1000):
        """
        Evaluate log probabilities for given input-context pairs.
        :param X: a pair (x, y) where x rows are inputs and y rows are context variables
        :param sess: tensorflow session where the graph is run
        :param log: whether to return probabilities in the log domain
        :param batch_size: number of samples to be evaluated in one batch
        :return: log probabilities: log p(y|x)
        """
        
        if self.context_dim == 0:
            n = len(X)//batch_size
            lprob = []
            for i in range(n):
                lprob.append(sess.run(self.L,feed_dict={self.input:X[i*batch_size:(i+1)*batch_size]}))

            if n != len(X)/batch_size:
                lprob.append(sess.run(self.L,feed_dict={self.input:X[n*batch_size:]}))
            
        else:
            x, y = X
            n = len(y)//batch_size
            lprob = []
            for i in range(n):
                lprob.append(sess.run(self.L,feed_dict={self.input:y[i*batch_size:(i+1)*batch_size],
                                                        self.context:x[i*batch_size:(i+1)*batch_size]}))

            if n != len(y)/batch_size:
                lprob.append(sess.run(self.L,feed_dict={self.input:y[n*batch_size:],
                                                        self.context:x[n*batch_size:]}))
        
        lprob = np.row_stack(lprob)

        return lprob if log else np.exp(lprob)